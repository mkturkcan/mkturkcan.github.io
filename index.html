<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Mehmet Kerem Turkcan</title> <meta name="author" content="Mehmet Kerem Turkcan"> <meta name="description" content="Postdoctoral Research Scientist at Columbia University "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mkturkcan.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Mehmet Kerem</span> Turkcan </h1> <p class="desc">Associate Research Scientist in <a href="http://www.civil.columbia.edu/" target="_blank" rel="noopener noreferrer">Civil Engineering &amp; Engineering Mechanics</a> at <a href="http://www.columbia.edu/" target="_blank" rel="noopener noreferrer">Columbia University</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/prof_pic-480.webp 480w, /assets/img/prof_pic-800.webp 800w, /assets/img/prof_pic-1400.webp 1400w, " sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"></source> <img src="/assets/img/prof_pic.jpg?0b885db5fd10985003da5ff4262c1774" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <p>210E 2276 12th Avenue</p> <p>Columbia University</p> <p>New York, New York 10027</p> </div> </div> <div class="clearfix"> <p>Greetings! I am currently an associate research scientist in <a href="https://cs3-erc.org/" target="_blank" rel="noopener noreferrer">the Center for Smart Streetscapes (CS3)</a> and <a href="https://www.civil.columbia.edu/" target="_blank" rel="noopener noreferrer">Civil Engineering &amp; Engineering Mechanics</a> at <a href="http://www.columbia.edu/" target="_blank" rel="noopener noreferrer">Columbia University</a> specializing in computer vision and deep learning applications. Before, I was a postdoctoral research scientist in <a href="http://www.ee.columbia.edu/" target="_blank" rel="noopener noreferrer">Electrical Engineering</a>. My research focuses on real-world, low-latency deployment of object detection and trajectory prediction models, and retrieval-augmented generation via large language models. To test algorithms, I lead projects on large-scale data collection and annotation, and improve upon the state-of-the-art models to adapt them to the strict performance and quality requirements of real-world use cases. Right now, my focus is specifically on applications to urban streetscapes and robotic surgeries.</p> <p>In addition to research activities, I am an independent video game developer and movie maker. I publish games as <a href="https://wisedawn.itch.io/" rel="external nofollow noopener" target="_blank">Wisedawn</a>, and am part of <a href="https://www.youtube.com/@KEDIKAT" rel="external nofollow noopener" target="_blank">KEDIKAT</a>.</p> <p>Previously, as a member of the <a href="http://www.bionet.ee.columbia.edu/" rel="external nofollow noopener" target="_blank">Bionet</a> group, I studied the efficient initialization and execution of very-large-scale, real connectome and synaptome driven, single-synapse-level simulations of neural circuits on GPUs for the purpose of understanding realistic neural networks. Using the tools I developed, I worked on the design of computational circuits for understanding the mushroom body and the lateral horn, two brain regions in insects linked with associative and evolutionary memories. I am also one of the main developers of an open-source interactive neuroscientific computing platform called <a href="https://github.com/FlyBrainLab" target="_blank" rel="noopener noreferrer">FlyBrainLab</a> (FBL) that enables access to, manipulation of and simulations for fruit fly brain data.</p> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jul 15, 2025</th> <td> I will be the lead engineering mentor for the Center for Smart Streetscapes (CS3) Research Experience for Teachers program in 2025. This 4-week program will introduce the most recent emerging models and concepts in AI to high school teachers. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 23, 2025</th> <td> Our paper, “Adaptive Data Collection for Robust Learning Across Multiple Distributions”, has been accepted to ICML 2025! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 23, 2025</th> <td> Our paper, “Loosely Coupled Oscillators as a Correlate of Behavioral Control Circuits Within the Central Complex of the Fruit Fly”, is now available <a href="https://link.springer.com/chapter/10.1007/978-981-96-6576-1_16" rel="external nofollow noopener" target="_blank">online</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 22, 2025</th> <td> Our paper, “Distributed VLMs: Efficient Vision-Language Processing through Cloud-Edge Collaboration”, is now available <a href="https://www.computer.org/csdl/proceedings-article/percom-workshops/2025/355300a280/27FQPv91PAk" rel="external nofollow noopener" target="_blank">online</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 14, 2025</th> <td> Our paper, “Distributed VLMs: Efficient Vision-Language Processing through Cloud-Edge Collaboration”, has been accepted to PerCom 2025! </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/ffbo-480.webp 480w, /assets/img/publication_preview/ffbo-800.webp 800w, /assets/img/publication_preview/ffbo-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ffbo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ffbo.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ukani2019fruit" class="col-sm-8"> <div class="title">The Fruit Fly Brain Observatory: From Structure to Function</div> <div class="author"> Nikul H Ukani, Chung-Heng Yeh, Adam Tomkins, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Yiyin Zhou, Dorian Florescu, Carlos Luna Ortiz, Yu-Chi Huang, Cheng-Te Wang, Mehmet K Turkcan, Tingkai Liu, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>BioRxiv</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/580290" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.fruitflybrain.org/#/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The fruit fly is a key model organism for studying the activity of interconnected brain circuits. A large scattered global research community of neurobiologists and neurogeneticists, computational and theoretical neuroscientists, and computer scientists and engineers has been developing a vast trove of experimental and modeling data that has yet to be distilled into new knowledge and understanding of the functional logic of the brain. Developing open shared models, modelling tools and data repositories that can be accessed from anywhere in the world is the necessary engine for accelerating our understanding of how the brain works. To that end we developed the Fruit Fly Brain Observatory (FFBO), the next generation open-source platform to support open, collaborative Drosophila neuroscience research. FFBO provides a (i) hub for storing and integrating fruit fly brain research data from multiple data sources worldwide, (ii) unified repository of tools and methods to build, emulate and compare fruit fly brain models in health and disease, and (iii) an open framework for fruit fly brain data processing and model execution. FFBO provides access to application tools for visualizing, configuring, simulating and analyzing computational models of brain circuits of the (i) cell type map, (ii) connectome, (iii) synaptome, and (iv) activity map using intuitive queries in plain English. Tools are provided to extract the function inherent in these structural maps. All applications can be accessed with any modern browser.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/neuromynerva_example-480.webp 480w, /assets/img/publication_preview/neuromynerva_example-800.webp 800w, /assets/img/publication_preview/neuromynerva_example-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/neuromynerva_example.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neuromynerva_example.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lazar2021accelerating" class="col-sm-8"> <div class="title">Accelerating with FlyBrainLab the discovery of the functional logic of the Drosophila brain in the connectomic and synaptomic era</div> <div class="author"> Aurel A Lazar, Tingkai Liu, Mehmet Kerem Turkcan, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Yiyin Zhou' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Elife</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.7554/eLife.62362" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://flybrainlab.fruitflybrain.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In recent years, a wealth of Drosophila neuroscience data have become available including cell type and connectome/synaptome datasets for both the larva and adult fly. To facilitate integration across data modalities and to accelerate the understanding of the functional logic of the fruit fly brain, we have developed FlyBrainLab, a unique open-source computing platform that integrates 3D exploration and visualization of diverse datasets with interactive exploration of the functional logic of modeled executable brain circuits. FlyBrainLab’s User Interface, Utilities Libraries and Circuit Libraries bring together neuroanatomical, neurogenetic and electrophysiological datasets with computational models of different researchers for validation and comparison within the same platform. Seeking to transcend the limitations of the connectome/synaptome, FlyBrainLab also provides libraries for molecular transduction arising in sensory coding in vision/olfaction. Together with sensory neuron activity data, these libraries serve as entry points for the exploration, analysis, comparison, and evaluation of circuit functions of the fruit fly brain.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/frontiers-480.webp 480w, /assets/img/publication_preview/frontiers-800.webp 800w, /assets/img/publication_preview/frontiers-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/frontiers.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="frontiers.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lazar2022programmable" class="col-sm-8"> <div class="title">A Programmable Ontology Encompassing the Functional Logic of the Drosophila Brain</div> <div class="author"> Aurel A Lazar, Mehmet Kerem Turkcan, and Yiyin Zhou</div> <div class="periodical"> <em>Frontiers in Neuroinformatics</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/fninf.2022.853098" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The Drosophila brain has only a fraction of the number of neurons of higher organisms such as mice and humans. Yet the sheer complexity of its neural circuits recently revealed by large connectomics datasets suggests that computationally modeling the function of fruit fly brain circuits at this scale poses significant challenges. To address these challenges, we present here a programmable ontology that expands the scope of the current Drosophila brain anatomy ontologies to encompass the functional logic of the fly brain. The programmable ontology provides a language not only for modeling circuit motifs but also for programmatically exploring their functional logic. To achieve this goal, we tightly integrated the programmable ontology with the workflow of the interactive FlyBrainLab computing platform. As part of the programmable ontology, we developed NeuroNLP++, a web application that supports free-form English queries for constructing functional brain circuits fully anchored on the available connectome/synaptome datasets, and the published worldwide literature. In addition, we present a methodology for including a model of the space of odorants into the programmable ontology, and for modeling olfactory sensory circuits of the antenna of the fruit fly brain that detect odorant sources. Furthermore, we describe a methodology for modeling the functional logic of the antennal lobe circuit consisting of a massive number of local feedback loops, a characteristic feature observed across Drosophila brain regions. Finally, using a circuit library, we demonstrate the power of our methodology for interactively exploring the functional logic of the massive number of feedback loops in the antennal lobe.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/surg_mdpi-480.webp 480w, /assets/img/publication_preview/surg_mdpi-800.webp 800w, /assets/img/publication_preview/surg_mdpi-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/surg_mdpi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="surg_mdpi.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zang2023surgical" class="col-sm-8"> <div class="title">Surgical Phase Recognition in Inguinal Hernia Repair—AI-Based Confirmatory Baseline and Exploration of Competitive Models</div> <div class="author"> Chengbo Zang, Mehmet Kerem Turkcan, Sanjeev Narasimhan, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Yuqing Cao, Kaan Yarali, Zixuan Xiang, Skyler Szot, Feroz Ahmad, Sarah Choksi, Daniel P Bitner, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>Bioengineering</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/bioengineering10060654" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Video-recorded robotic-assisted surgeries allow the use of automated computer vision and artificial intelligence/deep learning methods for quality assessment and workflow analysis in surgical phase recognition. We considered a dataset of 209 videos of robotic-assisted laparoscopic inguinal hernia repair (RALIHR) collected from 8 surgeons, defined rigorous ground-truth annotation rules, then pre-processed and annotated the videos. We deployed seven deep learning models to establish the baseline accuracy for surgical phase recognition and explored four advanced architectures. For rapid execution of the studies, we initially engaged three dozen MS-level engineering students in a competitive classroom setting, followed by focused research. We unified the data processing pipeline in a confirmatory study, and explored a number of scenarios which differ in how the DL networks were trained and evaluated. For the scenario with 21 validation videos of all surgeons, the Video Swin Transformer model achieved  0.85 validation accuracy, and the Perceiver IO model achieved  0.84. Our studies affirm the necessity of close collaborative research between medical experts and engineers for developing automated surgical phase recognition models deployable in clinical settings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/streetnav-480.webp 480w, /assets/img/publication_preview/streetnav-800.webp 800w, /assets/img/publication_preview/streetnav-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/streetnav.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="streetnav.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jain2024streetnav" class="col-sm-8"> <div class="title">StreetNav: Leveraging street cameras to support precise outdoor navigation for blind pedestrians</div> <div class="author"> Gaurav Jain, Basel Hindi, Zihao Zhang, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Koushik Srinivasula, Mingyu Xie, Mahshid Ghasemi, Daniel Weiner, Sophie Ana Paris, Xin Yi Therese Xu, Michael Malcolm, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3654777" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Blind and low-vision (BLV) people rely on GPS-based systems for outdoor navigation. GPS’s inaccuracy, however, causes them to veer off track, run into obstacles, and struggle to reach precise destinations. While prior work has made precise navigation possible indoors via hardware installations, enabling this outdoors remains a challenge. Interestingly, many outdoor environments are already instrumented with hardware such as street cameras. In this work, we explore the idea of repurposing existing street cameras for outdoor navigation. Our community-driven approach considers both technical and sociotechnical concerns through engagements with various stakeholders: BLV users, residents, business owners, and Community Board leadership. The resulting system, StreetNav, processes a camera’s video feed using computer vision and gives BLV pedestrians real-time navigation assistance. Our evaluations show that StreetNav guides users more precisely than GPS, but its technical performance is sensitive to environmental occlusions and distance from the camera. We discuss future implications for deploying such systems at scale.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/boundless-480.webp 480w, /assets/img/publication_preview/boundless-800.webp 800w, /assets/img/publication_preview/boundless-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/boundless.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="boundless.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="turkcan2024boundlessgeneratingphotorealisticsynthetic" class="col-sm-8"> <div class="title">Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes</div> <div class="author"> Mehmet Kerem Turkcan, Yuyang Li, Chengbo Zang, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Javad Ghaderi, Gil Zussman, Zoran Kostic' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2409.03022" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We introduce Boundless, a photo-realistic synthetic data generation system for enabling highly accurate object detection in dense urban streetscapes. Boundless can replace massive real-world data collection and manual ground-truth object annotation (labeling) with an automated and configurable process. Boundless is based on the Unreal Engine 5 (UE5) City Sample project with improvements enabling accurate collection of 3D bounding boxes across different lighting and scene variability conditions. We evaluate the performance of object detection models trained on the dataset generated by Boundless when used for inference on a real-world dataset acquired from medium-altitude cameras. We compare the performance of the Boundless-trained model against the CARLA-trained model and observe an improvement of 7.8 mAP. The results we achieved support the premise that synthetic data generation is a credible methodology for training/fine-tuning scalable object detection models for urban scenes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/constellation2-480.webp 480w, /assets/img/publication_preview/constellation2-800.webp 800w, /assets/img/publication_preview/constellation2-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/constellation2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="constellation2.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="turkcan2024constellationdatasetbenchmarkinghighaltitude" class="col-sm-8"> <div class="title">Constellation Dataset: Benchmarking High-Altitude Object Detection for an Urban Intersection</div> <div class="author"> Mehmet Kerem Turkcan, Sanjeev Narasimhan, Chengbo Zang, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Gyung Hyun Je, Bo Yu, Mahshid Ghasemi, Javad Ghaderi, Gil Zussman, Zoran Kostic' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2404.16944" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://mkturkcan.github.io/constellation-web/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>We introduce Constellation, a dataset of 13K images suitable for research on detection of objects in dense urban streetscapes observed from high-elevation cameras, collected for a variety of temporal conditions. The dataset addresses the need for curated data to explore problems in small object detection exemplified by the limited pixel footprint of pedestrians observed tens of meters from above. It enables the testing of object detection models for variations in lighting, building shadows, weather, and scene dynamics. We evaluate contemporary object detection architectures on the dataset, observing that state-of-the-art methods have lower performance in detecting small pedestrians compared to vehicles, corresponding to a 10% difference in average precision (AP). Using structurally similar datasets for pretraining the models results in an increase of 1.8% mean AP (mAP). We further find that incorporating domain-specific data augmentations helps improve model performance. Using pseudo-labeled data, obtained from inference outcomes of the best-performing models, improves the performance of the models. Finally, comparing the models trained using the data collected in two different time intervals, we find a performance drift in models due to the changes in intersection conditions over time. The best-performing model achieves a pedestrian AP of 92.0% with 11.5 ms inference time on NVIDIA A100 GPUs, and an mAP of 95.4%.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/edm-480.webp 480w, /assets/img/publication_preview/edm-800.webp 800w, /assets/img/publication_preview/edm-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/edm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="edm.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="2024.EDM-posters.68" class="col-sm-8"> <div class="title">Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors</div> <div class="author"> Blake Castleman, and Mehmet Kerem Turkcan</div> <div class="periodical"> <em>In Proceedings of the 17th International Conference on Educational Data Mining</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.5281/zenodo.12729908" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Recent advancements in large language models (LLMs) have facilitated the development of chatbots with sophisticated conversational capabilities. However, LLMs exhibit frequent inaccurate responses to queries, hindering applications in educational settings. In this paper, we investigate the effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors to increase response reliability. To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system. We then detail an evaluation, where student participants were presented with questions about the artificial intelligence curriculum to respond to. GPT-4 intelligent tutors with varying hierarchies of KB access and human domain experts then assessed these responses. Lastly, students cross-examined the intelligent tutors’ responses to the domain experts’ and ranked their various pedagogical abilities. Results suggest that, although these intelligent tutors still demonstrate a lower accuracy compared to domain experts, the accuracy of the intelligent tutors increases when access to a KB is granted. We also observe that the intelligent tutors with KB access exhibit better pedagogical abilities to speak like a teacher and understand students than those of domain experts, while their ability to help students remains lagging behind domain experts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/carladt-480.webp 480w, /assets/img/publication_preview/carladt-800.webp 800w, /assets/img/publication_preview/carladt-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/carladt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="carladt.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="fu2024digital" class="col-sm-8"> <div class="title">Digital twin for pedestrian safety warning at a single urban traffic intersection</div> <div class="author"> Yongjie Fu, Mehmet K Turkcan, Vikram Anantha, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Zoran Kostic, Gil Zussman, Xuan Di' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2024 IEEE Intelligent Vehicles Symposium (IV)</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/IV55156.2024.10588544" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Ensuring the safety of Vulnerable Road Users (VRUs) at intersections is crucial to enhancing urban traffic systems. This paper introduces a novel intelligent warning system specifically designed to increase the safety of VRUs crossing intersections. The proposed system leverages the COSMOS testbed to obtain real time vehicle information and employs Message Queuing Telemetry Transport (MQTT) as a standards-based messaging protocol for device communication and data transmission and utilizes a transformer model and Time To Collision (TTC) method to predict the collision. To validate the effectiveness and reliability of our intelligent alert system, we conducted comprehensive tests using the CARLA simulator, incorporating hardware in the loop simulation approach. The results demonstrate the potential for increased situational awareness and reduced risk factors associated with VRUs at intersections. Our work supports the integration of this intelligent alert system as a viable solution for reducing accidents and enhancing the overall safety of urban intersections in real time.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%6B%74%32%31%32%36@%63%6F%6C%75%6D%62%69%61.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=306TgWoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/mkturkcan" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://twitter.com/mkturkcan" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note"> You can contact me through my email. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Mehmet Kerem Turkcan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>